{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import copy\n",
    "import pickle\n",
    "import ml_collections\n",
    "import wandb, signatory\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path as pt\n",
    "from tqdm import tqdm\n",
    "sns.set_style(\"darkgrid\")  # 원하는 스타일 선택\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from src.utils import *\n",
    "from src.evaluation.summary import full_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration dict\n",
    "config_dir = 'configs/config.yaml'\n",
    "with open(config_dir) as file:\n",
    "    config = ml_collections.ConfigDict(yaml.safe_load(file))\n",
    "\n",
    "if (config.device == \"cuda\" and torch.cuda.is_available()):\n",
    "    config.update({\"device\": \"cuda:1\"}, allow_val_change=True)\n",
    "    torch.cuda.set_device(0)\n",
    "else:\n",
    "    config.update({\"device\": \"cpu\"}, allow_val_change=True)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(data, window_size):\n",
    "    n_windows = data.shape[0] - window_size + 1\n",
    "    windows = np.zeros((n_windows, window_size, data.shape[1]))\n",
    "    for idx in range(n_windows):\n",
    "        windows[idx] = data[idx:idx + window_size]\n",
    "    return windows\n",
    "\n",
    "# Step 1: Load and preprocess data\n",
    "df = pd.read_csv(f\"./data/{config.file_name}.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "df.set_index('Date', inplace=True)\n",
    "df = df.apply(pd.to_numeric).astype(float)\n",
    "\n",
    "# Step 2: Compute log returns\n",
    "log_returns = np.diff(np.log(df), axis=0)\n",
    "print(log_returns.shape)\n",
    "\n",
    "# Step 3: Scale the log returns\n",
    "log_returns_scaled, scalers = scaling(log_returns)\n",
    "\n",
    "# Step 4: Prepare initial prices and create rolling windows\n",
    "init_price = torch.from_numpy(np.array(df)[:-(config.n_steps), :]).float().unsqueeze(1)\n",
    "log_returns_scaled = torch.from_numpy(rolling_window(log_returns_scaled, config.n_steps)).float()\n",
    "print('init_price:', init_price.shape)\n",
    "print('log_returns_scaled:', log_returns_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative models for time series generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the data into training and validation set for the offline evaluation of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(log_returns.shape[0] * 0.6)\n",
    "\n",
    "training_data = log_returns_scaled[:train_size]\n",
    "train_init_price = init_price[:train_size]\n",
    "test_data = log_returns_scaled[train_size:]\n",
    "test_init_price = init_price[train_size:]\n",
    "\n",
    "print(\"training_data: \", training_data.shape)\n",
    "print(\"test_data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = TensorDataset(training_data)\n",
    "test_set = TensorDataset(test_data)\n",
    "\n",
    "train_dl = DataLoader(training_set, batch_size=config.batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_set, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct a generator and a discriminator for this task. Both the generator and discriminator takes as input the time series. Then we have the training algorithm TailGANTrainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the generator, discriminator and the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.baselines.networks.discriminators import UserDiscriminator\n",
    "from src.baselines.networks.generators import UserGenerator\n",
    "from src.baselines.trainer import *\n",
    "\n",
    "generator = UserGenerator(config)\n",
    "discriminator = UserDiscriminator(config)\n",
    "trainer = GANTrainer(G=generator, D=discriminator, train_dl=train_dl, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model training\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVFIT-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy, wasserstein_distance, ks_2samp, spearmanr, kendalltau\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "full_name = \"indices_300_256_G_lr_0.0002_D_lr_0.0001_noise_6_hidden_dim_120_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Find the best epochs based on 100 days cumulative distribution \"\"\"\n",
    "window = 100\n",
    "gen_tmp = UserGenerator(config)\n",
    "min_dist = float('inf')  \n",
    "best_epoch = 0\n",
    "\n",
    "def compute_avg_emd(real_data, fake_data, window):\n",
    "    \"\"\"\n",
    "    Compute the Earth Mover's Distance (EMD) for real and fake data over a rolling window.\n",
    "    \"\"\"\n",
    "    emd = 0\n",
    "    for i in range(real_data.shape[1]):  # Iterate over features\n",
    "        real_dist = rolling_window(real_data[:, i, :].T, window).sum(axis=1).ravel()\n",
    "        fake_dist = rolling_window(fake_data[:, i, :].T, window).sum(axis=1).ravel()\n",
    "        emd += wasserstein_distance(real_dist, fake_dist)\n",
    "    return emd\n",
    "\n",
    "batch_size = 1000\n",
    "for epoch in range(50, 300, 5):\n",
    "    \n",
    "    # Load generator for the current epoch\n",
    "    generator.load_state_dict(torch.load(f'./results/models/{full_name}/Generator_{epoch}.pt'))\n",
    "    generator.to(config.device)\n",
    "    generator.eval()\n",
    "    \n",
    "    # Generate fake data\n",
    "    noise = torch.randn(batch_size, config.noise_dim, config.n_steps, device=config.device)\n",
    "    with torch.no_grad():        \n",
    "        fake= generator(noise)                \n",
    "        \n",
    "    # Inverse scaling for real and fake data\n",
    "    print(fake.shape)\n",
    "    print(training_data.transpose(1, 2).shape)\n",
    "    fake_data = inverse_scaling(fake, scalers)        \n",
    "    real_data = inverse_scaling(training_data.transpose(1, 2), scalers)\n",
    "    \n",
    "    # Compute EMD\n",
    "    emd = compute_avg_emd(real_data, fake_data, window)\n",
    "\n",
    "    # Update best epoch if current EMD is lower\n",
    "    if emd < min_dist:\n",
    "        min_dist = emd\n",
    "        best_epoch = epoch\n",
    "        print(f\"min_distance: {min_dist:.3f}, best_epoch: {best_epoch}\") \n",
    "\n",
    "generator.load_state_dict(torch.load(f'./results/models/{full_name}/Generator_{best_epoch}.pt'))\n",
    "generator.to(config.device)\n",
    "generator.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Fake data와 Test data를 비교 \"\"\"\n",
    "batch_size = 6000\n",
    "noise = torch.randn(batch_size, config.noise_dim, config.n_steps, device=config.device)\n",
    "with torch.no_grad():\n",
    "    fake = generator(noise)\n",
    "\n",
    "fake_data = inverse_scaling(fake, scalers)\n",
    "real_data = inverse_scaling(test_data.transpose(1, 2), scalers)\n",
    "\n",
    "# 각 feature의 전체 최소값 및 최대값 계산\n",
    "min_vals = real_data.min(axis=(0, 2), keepdims=True)  # Shape: (1, 5, 1)\n",
    "max_vals = real_data.max(axis=(0, 2), keepdims=True)  # Shape: (1, 5, 1)\n",
    "\n",
    "# 마스크 계산 (모든 샘플이 범위 내에 있는지 확인)\n",
    "mask = np.all((fake_data >= min_vals * 1.5) & (fake_data <= max_vals * 1.5), axis=(1, 2))\n",
    "\n",
    "# 마스크를 적용하여 fake_data 필터링\n",
    "print(real_data.shape, type(real_data))\n",
    "print(fake_data.shape, type(fake_data))\n",
    "fake_data = fake_data[mask]\n",
    "fake_data = fake_data[:5000]\n",
    "print(f\"Filtered shape: {fake_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Visualize the distribution of the real and fake data \"\"\"\n",
    "fake_list = [fake_data[:, i, :] for i in range(fake_data.shape[1])]\n",
    "real_list = [real_data[:, i, :] for i in range(real_data.shape[1])]\n",
    "\n",
    "# Plot the distribution of the real and fake data\n",
    "windows = [1, 20, 100]\n",
    "for j in range(config.n_vars):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(18, 4))  \n",
    "\n",
    "    for i in range(len(windows)):\n",
    "        col = i\n",
    "\n",
    "        real_dist = rolling_window(real_list[j].T, windows[i]).sum(axis=1).ravel()\n",
    "        fake_dist = rolling_window(fake_list[j].T, windows[i]).sum(axis=1).ravel()        \n",
    "        \n",
    "        min_val = real_dist.min()\n",
    "        max_val = real_dist.max()\n",
    "        \n",
    "        bins = np.linspace(min_val, max_val, 81)  \n",
    "        \n",
    "        sns.histplot(real_dist, bins=bins, kde=False, ax=axs[col], color='tab:blue', alpha=0.5, stat='density')\n",
    "        sns.histplot(fake_dist, bins=bins, kde=False, ax=axs[col], color='tab:orange', alpha=0.5, stat='density')\n",
    "\n",
    "        axs[col].set_xlim(*np.quantile(real_dist, [0.001, .999]))\n",
    "        \n",
    "        axs[col].set_title('{} day return distribution'.format(windows[i]), size=18)\n",
    "        axs[col].yaxis.grid(True, alpha=0.5)\n",
    "        axs[col].set_xlabel('Cumulative log return', fontsize=12)\n",
    "        axs[col].set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "    axs[0].legend(['Historical returns', 'Synthetic returns'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "def calculate_distribution_scores(real, fake, num_G, windows):\n",
    "    scores = {\n",
    "        'EMD': np.zeros((num_G, len(windows))),\n",
    "        'KL': np.zeros((num_G, len(windows))),\n",
    "        'JS': np.zeros((num_G, len(windows))),\n",
    "        'KS': np.zeros((num_G, len(windows)))\n",
    "    }\n",
    "\n",
    "    for i in range(num_G):\n",
    "        for j in range(len(windows)):\n",
    "            real_dist = rolling_window(real[i].T, windows[j]).sum(axis=1).ravel()\n",
    "            fake_dist = rolling_window(fake[i].T, windows[j]).sum(axis=1).ravel()\n",
    "            \n",
    "            np.random.shuffle(real_dist)\n",
    "            np.random.shuffle(fake_dist)\n",
    "            \n",
    "            # Calculate EMD\n",
    "            scores['EMD'][i, j] = wasserstein_distance(real_dist, fake_dist)\n",
    "            \n",
    "            # Calculate KS Statistic\n",
    "            scores['KS'][i, j], _ = ks_2samp(real_dist, fake_dist)                                    \n",
    "            \n",
    "            # Create histograms to estimate the probability distributions\n",
    "            real_hist, bin_edges = np.histogram(real_dist, bins=100, density=True)\n",
    "            fake_hist, _ = np.histogram(fake_dist, bins=bin_edges, density=True)\n",
    "            \n",
    "            # Normalize the histograms to get probability distributions\n",
    "            real_prob = real_hist / np.sum(real_hist)\n",
    "            fake_prob = fake_hist / np.sum(fake_hist)\n",
    "            \n",
    "            # Calculate KL Divergence\n",
    "            kl_divergence = entropy(real_prob + 1e-10, fake_prob + 1e-10)\n",
    "            scores['KL'][i, j] = kl_divergence\n",
    "\n",
    "            # Calculate JS Divergence\n",
    "            js_divergence = jensenshannon(real_prob + 1e-10, fake_prob + 1e-10)\n",
    "            scores['JS'][i, j] = js_divergence\n",
    "                \n",
    "    df_scores = {}\n",
    "    for metric, data in scores.items():\n",
    "        data = np.round(data, decimals=4)\n",
    "        df_scores[metric] = pd.DataFrame(data.T, index=windows, columns=[f'{metric} {i}' for i in range(num_G)])\n",
    "        \n",
    "    emd_avg = np.mean(scores['EMD'], axis=0)\n",
    "    df_scores['EMD']['EMD_avg'] = np.round(emd_avg, decimals=4)\n",
    "    \n",
    "    return df_scores\n",
    "\n",
    "# Calculate the distribution scores\n",
    "windows = pd.Series([1, 5, 20, 100], name='window size')\n",
    "\n",
    "results_emd = calculate_distribution_scores(real_list, fake_list, config.n_vars, windows)\n",
    "results_emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def calculate_skew_kurt_diff(fake_list, real_list):\n",
    "    results = []\n",
    "    skewness_diffs = []\n",
    "    kurtosis_diffs = []\n",
    "    \n",
    "    for asset_idx in range(len(real_list)):\n",
    "        fake_data = fake_list[asset_idx]\n",
    "        real_data = real_list[asset_idx]\n",
    "\n",
    "        # 각 샘플에 대해 skewness와 kurtosis를 계산하고 평균을 구합니다.\n",
    "        fake_skewness = np.mean([skew(sample) for sample in fake_data])\n",
    "        fake_kurtosis = np.mean([kurtosis(sample, fisher=False) for sample in fake_data])\n",
    "        print(fake_kurtosis)\n",
    "        real_skewness = np.mean([skew(sample) for sample in real_data])\n",
    "        real_kurtosis = np.mean([kurtosis(sample, fisher=False) for sample in real_data])\n",
    "        print(real_kurtosis)\n",
    "\n",
    "        # skewness와 kurtosis의 차이 계산\n",
    "        skewness_diff = np.abs(real_skewness - fake_skewness)\n",
    "        kurtosis_diff = np.abs(real_kurtosis - fake_kurtosis)\n",
    "\n",
    "        # 소수점 자릿수 제한\n",
    "        skewness_diff = round(skewness_diff, 4)\n",
    "        kurtosis_diff = round(kurtosis_diff, 4)\n",
    "\n",
    "        results.append((skewness_diff, kurtosis_diff))\n",
    "        skewness_diffs.append(skewness_diff)\n",
    "        kurtosis_diffs.append(kurtosis_diff)\n",
    "\n",
    "        print(f'Asset {asset_idx + 1} Skewness Difference: {skewness_diff}')\n",
    "        print(f'Asset {asset_idx + 1} Kurtosis Difference: {kurtosis_diff}')\n",
    "        \n",
    "    # 평균 계산 수정\n",
    "    avg_skewness_diff = round(np.mean(skewness_diffs), 4)\n",
    "    avg_kurtosis_diff = round(np.mean(kurtosis_diffs), 4)\n",
    "\n",
    "    print(f\"\\n[평균] Skewness Difference: {avg_skewness_diff}\")\n",
    "    print(f\"[평균] Kurtosis Difference: {avg_kurtosis_diff}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "calculate_skew_kurt_diff(fake_list, real_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" visualizing acf plots \"\"\"\n",
    "def plot_acf_comparison(real_list, fake_list, n_vars, lags=30):    \n",
    "    data_types = ['Identity', 'Squared']\n",
    "    data_transforms = [lambda x: x, np.square]  \n",
    "    titles = ['Identity log returns', 'Squared log returns']\n",
    "\n",
    "    for i in range(n_vars):\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))  \n",
    "        print(i)\n",
    "        for ax, data_type, transform, title in zip(axs, data_types, data_transforms, titles):\n",
    "            \n",
    "            transformed_real = transform(real_list[i])\n",
    "            transformed_fake = transform(fake_list[i])\n",
    "            \n",
    "            acf_real = np.array([acf(ts, nlags=lags) for ts in transformed_real])\n",
    "            acf_fake = np.array([acf(ts, nlags=lags) for ts in transformed_fake])\n",
    "            \n",
    "            mean_real = np.mean(acf_real, axis=0)\n",
    "            std_real = np.std(acf_real, axis=0)\n",
    "            mean_fake = np.mean(acf_fake, axis=0)\n",
    "            std_fake = np.std(acf_fake, axis=0)\n",
    "                        \n",
    "            ax.plot(mean_real, label=f'{data_type} ACF Real - Mean', color='tab:blue')\n",
    "            ax.fill_between(range(lags+1), mean_real - 0.5*std_real, mean_real + 0.5*std_real, color='tab:blue', alpha=0.2,\n",
    "                            label=f'{data_type} ACF Real - 1/2 Std Dev')        \n",
    "            ax.plot(mean_fake, label=f'{data_type} ACF Synthetic - Mean', color='tab:orange')\n",
    "            ax.fill_between(range(lags+1), mean_fake - 0.5*std_fake, mean_fake + 0.5*std_fake, color='tab:orange', alpha=0.2,\n",
    "                            label=f'{data_type} ACF Synthetic - 1/2 Std Dev')\n",
    "            \n",
    "            ax.set_ylim(-0.15, 0.3)\n",
    "            ax.set_title(title)\n",
    "            ax.grid(True)\n",
    "            ax.axhline(y=0, color='k')\n",
    "            ax.axvline(x=0, color='k')\n",
    "            ax.set_xlabel('Lag (number of days)', fontsize=16)\n",
    "            \n",
    "        axs[0].legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "plot_acf_comparison(real_list, fake_list, config.n_vars, lags=30)\n",
    "\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def calculate_acf_score_mean_vs_mean_no_std(\n",
    "    real_list,\n",
    "    fake_list,\n",
    "    lags=30,\n",
    "    loss_type='mse'\n",
    "):\n",
    "    \"\"\"\n",
    "    real_list[i], fake_list[i] 각각 하나의 \"그룹\" 또는 \"자산\" 집합이라고 가정\n",
    "    각 real_list[i]에는 (n_real_samples_i, ) 형태의 시계열이 여러 개 들어있다고 보고,\n",
    "    fake_list[i]에도 여러 시계열이 들어있다고 봄.\n",
    "\n",
    "    -> Real 평균 ACF vs. Fake 평균 ACF만을 비교하여 MSE, MAE 등을 계산하는 버전.\n",
    "       표준편차( std )는 따로 계산하지 않음.\n",
    "    \"\"\"\n",
    "    # 변환 함수 정의\n",
    "    data_transforms = [lambda x: x, np.abs, np.square]  \n",
    "    titles = ['Identity log returns', 'Absolute log returns', 'Squared log returns']\n",
    "\n",
    "    acf_scores = {}  # 그룹별 결과 저장\n",
    "    # 최종적으로 transform별 \"평균 점수(mean of means)\"를 계산할 목적\n",
    "    avg_scores_per_transform = {title: [] for title in titles}\n",
    "\n",
    "    n_vars = len(real_list)\n",
    "    for group_idx in range(n_vars):\n",
    "        group_scores = {}  # 이 그룹에 대한 transform별 점수(스칼라값)\n",
    "\n",
    "        for transform, title in zip(data_transforms, titles):\n",
    "            # (1) Real / Fake 데이터 변환\n",
    "            transformed_real = transform(real_list[group_idx])\n",
    "            transformed_fake = transform(fake_list[group_idx])\n",
    "            \n",
    "            # (2) 각 샘플별 ACF 계산\n",
    "            acf_real = np.array([acf(ts, nlags=lags) for ts in transformed_real])  \n",
    "            acf_fake = np.array([acf(ts, nlags=lags) for ts in transformed_fake])  \n",
    "\n",
    "            # (3) Real, Fake 각각의 평균 ACF 계산\n",
    "            mean_acf_real = np.mean(acf_real, axis=0)  # shape=(lags+1,)\n",
    "            mean_acf_fake = np.mean(acf_fake, axis=0)  # shape=(lags+1,)\n",
    "\n",
    "            # (4) Real 평균 ACF vs. Fake 평균 ACF 차이\n",
    "            diff = mean_acf_fake - mean_acf_real\n",
    "\n",
    "            # (5) loss_type에 따라 계산 (MSE or MAE 등)\n",
    "            if loss_type == 'mse':\n",
    "                val = np.mean(diff**2)\n",
    "            elif loss_type == 'mae':\n",
    "                val = np.mean(np.abs(diff))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "\n",
    "            # (6) 표준편차는 사용하지 않으므로, 결과는 스칼라 val만\n",
    "            group_scores[title] = round(val, 4)\n",
    "\n",
    "            # (7) 전체 평균 계산을 위해 저장\n",
    "            avg_scores_per_transform[title].append(val)\n",
    "\n",
    "        # 해당 그룹에 대한 transform별 점수 기록\n",
    "        acf_scores[f\"Group {group_idx+1}\"] = group_scores\n",
    "    \n",
    "    # 모든 그룹에 대한 “평균의 평균”(mean of means) 계산\n",
    "    overall_average_scores = {}\n",
    "    for title in titles:\n",
    "        mean_of_means = np.mean(avg_scores_per_transform[title])\n",
    "        overall_average_scores[title] = round(mean_of_means, 4)\n",
    "\n",
    "    return acf_scores, overall_average_scores\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "lags = 30\n",
    "loss_type = 'mse'  # 'mse', 'mae' 등\n",
    "\n",
    "acf_scores, overall_avg_scores = calculate_acf_score_mean_vs_mean_no_std(\n",
    "    real_list, \n",
    "    fake_list, \n",
    "    lags=lags, \n",
    "    loss_type=loss_type\n",
    ")\n",
    "\n",
    "print(\"=== ACF Scores (per group) ===\")\n",
    "for group_name, transform_scores in acf_scores.items():\n",
    "    print(f\"{group_name}:\")\n",
    "    for transform_title, val in transform_scores.items():\n",
    "        print(f\"  - {transform_title}: loss={val}\")\n",
    "\n",
    "print(\"\\n=== Overall Average Scores ===\")\n",
    "for transform_title, mean_of_means in overall_avg_scores.items():\n",
    "    print(f\"{transform_title}: mean of means={mean_of_means}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_mean(data):\n",
    "    \"\"\"\n",
    "    각 샘플의 상관행렬을 계산한 후, 그 평균을 반환.\n",
    "    \n",
    "    Parameters:\n",
    "        data (np.ndarray): shape = (num_samples, num_time_steps, num_features)\n",
    "    \n",
    "    Returns:\n",
    "        mean_correlation_matrix (np.ndarray): shape = (num_features, num_features)\n",
    "    \"\"\"\n",
    "    num_samples = data.shape[0]\n",
    "    correlations = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sample = data[i]  # shape = (num_time_steps, num_features)\n",
    "        corr_mat = np.corrcoef(sample, rowvar=False)\n",
    "        correlations.append(corr_mat)\n",
    "\n",
    "    mean_correlation_matrix = np.mean(correlations, axis=0)  \n",
    "    return mean_correlation_matrix\n",
    "\n",
    "\n",
    "def correlation_loss(real_mean_corr, fake_mean_corr, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    Real과 Fake의 평균 상관행렬 간의 손실을 계산.\n",
    "    \n",
    "    Parameters:\n",
    "        real_mean_corr (np.ndarray): Real 데이터의 평균 상관행렬\n",
    "        fake_mean_corr (np.ndarray): Fake 데이터의 평균 상관행렬\n",
    "        loss_type (str): 'mse', 'mae', 'frobenius' 중 선택\n",
    "    \n",
    "    Returns:\n",
    "        loss (float): 지정된 손실 타입에 따른 손실 값 (소수점 4자리로 반올림)\n",
    "    \"\"\"\n",
    "    diff = fake_mean_corr - real_mean_corr\n",
    "\n",
    "    if loss_type == 'mse':\n",
    "        loss_val = np.mean(diff**2)\n",
    "    elif loss_type == 'mae':\n",
    "        loss_val = np.mean(np.abs(diff))\n",
    "    elif loss_type == 'frobenius':            \n",
    "        loss_val = np.linalg.norm(diff, ord='fro')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss_type: {loss_type}\")\n",
    "\n",
    "    return round(loss_val, 4)\n",
    "\n",
    "# 상관행렬 계산을 위해 데이터 전치 (shape: (num_samples, num_time_steps, num_features))\n",
    "real_mean_corr = correlation_mean(np.transpose(real_data, (0, 2, 1)))\n",
    "fake_mean_corr = correlation_mean(np.transpose(fake_data, (0, 2, 1)))\n",
    "\n",
    "# 손실 계산\n",
    "loss_mae = correlation_loss(real_mean_corr, fake_mean_corr, loss_type='mae')\n",
    "loss_frob = correlation_loss(real_mean_corr, fake_mean_corr, loss_type='frobenius')\n",
    "loss_mse = correlation_loss(real_mean_corr, fake_mean_corr, loss_type='mse')\n",
    "\n",
    "# 결과 출력\n",
    "print(\"[Correlation Loss - MAE]\")\n",
    "print(f\"Loss: {loss_mae}\")\n",
    "\n",
    "print(\"\\n[Correlation Loss - Frobenius]\")\n",
    "print(f\"Loss: {loss_frob}\")\n",
    "\n",
    "print(\"\\n[Correlation Loss - MSE]\")\n",
    "print(f\"Loss: {loss_mse}\")\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(correlation_matrix, title, feature_names):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='cubehelix_r', linewidths=.5, ax=ax, fmt=\".2f\", annot_kws={\"size\": 12}, vmin=-0.1, vmax=1)\n",
    "    \n",
    "    # 축 이름 설정\n",
    "    ax.set_xticklabels(feature_names, fontsize=12)\n",
    "    ax.set_yticklabels(feature_names, fontsize=12)\n",
    "    \n",
    "    # 제목 및 시각적 요소\n",
    "    ax.set_title(title, fontsize=15)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "feature_names = ['DJI', 'IXIC', 'JPM', 'HSI', 'Gold', 'WTI']\n",
    "plot_correlation_heatmap(real_mean_corr, \"Real Data\", feature_names)\n",
    "plot_correlation_heatmap(fake_mean_corr, \"Fake Data\", feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "\n",
    "def ccf_single_sample(x, max_lag=10):\n",
    "    \"\"\"\n",
    "    x: shape = (time_steps, features)\n",
    "    - statsmodels.tsa.stattools.ccf(x[:, i], x[:, j])를 이용해\n",
    "      i, j 모든 쌍에 대해 lag=0..max_lag 범위의 cross-correlation을 구한다.\n",
    "    - 결과를 3D 배열 (features, features, max_lag+1)에 저장하여 반환.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cc_matrix: np.ndarray\n",
    "      shape = (num_features, num_features, max_lag+1)\n",
    "      cc_matrix[i, j, k] = ccf 결과에서 lag=k인 상관계수\n",
    "    \"\"\"\n",
    "    time_steps, num_features = x.shape\n",
    "    cc_matrix = np.zeros((num_features, num_features, max_lag+1), dtype=float)\n",
    "\n",
    "    for i in range(num_features):\n",
    "        for j in range(num_features):\n",
    "            # statsmodels ccf: lag >= 0 에 대한 상관계수\n",
    "            c_array = ccf(x[:, i], x[:, j], adjusted=False)\n",
    "            # lag=0 ~ max_lag만 추출\n",
    "            c_array = c_array[:(max_lag+1)]\n",
    "            # ccf 함수는 무한대로 확장될 수 있으므로, 필요한 max_lag+1까지만 자르기\n",
    "            if len(c_array) < (max_lag+1):\n",
    "                # 부족한 경우 0으로 채우기\n",
    "                c_array = np.pad(c_array, (0, max_lag+1 - len(c_array)), 'constant')\n",
    "            cc_matrix[i, j, :] = c_array\n",
    "\n",
    "    return cc_matrix\n",
    "\n",
    "def ccf_mean(data, max_lag=10):\n",
    "    \"\"\"\n",
    "    data: shape = (num_samples, time_steps, features)\n",
    "\n",
    "    각 샘플마다 ccf_single_sample -> (features, features, max_lag+1)\n",
    "    모든 샘플에 대해 평균낸 결과를 반환.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean_ccf: np.ndarray\n",
    "      shape = (num_features, num_features, max_lag+1)\n",
    "    \"\"\"\n",
    "    num_samples, time_steps, num_features = data.shape\n",
    "\n",
    "    # 모든 샘플의 CCF 결과를 쌓아두기\n",
    "    all_cc = np.zeros((num_samples, num_features, num_features, max_lag+1), dtype=float)\n",
    "\n",
    "    for s_idx in range(num_samples):\n",
    "        cc_matrix = ccf_single_sample(data[s_idx], max_lag=max_lag)\n",
    "        all_cc[s_idx] = cc_matrix  # shape=(features, features, max_lag+1)\n",
    "\n",
    "    # 샘플 차원에 대해 평균\n",
    "    mean_ccf = all_cc.mean(axis=0)  # shape=(features, features, max_lag+1)\n",
    "    return mean_ccf\n",
    "\n",
    "def ccf_loss_mean_vs_mean(real_mean_ccf, fake_mean_ccf, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    - real_mean_ccf: shape = (features, features, max_lag+1)\n",
    "      Real 데이터의 평균 CCF 행렬\n",
    "    - fake_mean_ccf: shape = (features, features, max_lag+1)\n",
    "      Fake 데이터의 평균 CCF 행렬\n",
    "    - loss_type: 'mse', 'mae', 'frobenius' 중 선택\n",
    "\n",
    "    returns: loss (float), 소수점 4자리로 반올림\n",
    "    \"\"\"\n",
    "    # 두 평균 CCF 행렬의 차이\n",
    "    diff = fake_mean_ccf - real_mean_ccf\n",
    "\n",
    "    # 손실 계산\n",
    "    if loss_type == 'mse':\n",
    "        loss_val = np.mean(diff**2)\n",
    "    elif loss_type == 'mae':\n",
    "        loss_val = np.mean(np.abs(diff))\n",
    "    elif loss_type == 'frobenius':\n",
    "        # 3D 배열에 대한 Frobenius norm\n",
    "        loss_val = np.linalg.norm(diff)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss_type: {loss_type}\")\n",
    "\n",
    "    return round(loss_val, 4)\n",
    "\n",
    "# 사용 예시\n",
    "max_lag = 10\n",
    "\n",
    "# CCF 계산을 위해 데이터 전치 (shape: (num_samples, time_steps, num_features))\n",
    "# 기존에 사용된 np.transpose(real_data, (0, 2, 1))은 이미 올바른 형상으로 가정\n",
    "real_mean_ccf = ccf_mean(np.transpose(real_data, (0, 2, 1)), max_lag=max_lag)\n",
    "fake_mean_ccf = ccf_mean(np.transpose(fake_data, (0, 2, 1)), max_lag=max_lag)\n",
    "\n",
    "# 손실 계산\n",
    "loss_mae = ccf_loss_mean_vs_mean(real_mean_ccf, fake_mean_ccf, loss_type='mae')\n",
    "loss_frob = ccf_loss_mean_vs_mean(real_mean_ccf, fake_mean_ccf, loss_type='frobenius')\n",
    "loss_mse = ccf_loss_mean_vs_mean(real_mean_ccf, fake_mean_ccf, loss_type='mse')\n",
    "\n",
    "# 결과 출력\n",
    "print(\"[Partial CCF Loss - MAE]\")\n",
    "print(f\"Loss: {loss_mae}\")\n",
    "\n",
    "print(\"\\n[Partial CCF Loss - Frobenius]\")\n",
    "print(f\"Loss: {loss_frob}\")\n",
    "\n",
    "print(\"\\n[Partial CCF Loss - MSE]\")\n",
    "print(f\"Loss: {loss_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leverage_effect_loss_mean_vs_mean(real_list, fake_list, lags=30, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    Real 데이터의 평균 레버리지 효과와 Fake 데이터의 평균 레버리지 효과를 비교하여 손실을 계산.\n",
    "\n",
    "    Parameters:\n",
    "        real_list (list of np.ndarray): 각 그룹별 Real 데이터. 각 요소는 shape=(n_real_samples, time_length)\n",
    "        fake_list (list of np.ndarray): 각 그룹별 Fake 데이터. 각 요소는 shape=(n_fake_samples, time_length)\n",
    "        lags (int): 최대 시차 (tau) 값\n",
    "        loss_type (str): 손실 함수의 종류 ('mse', 'mae', 'fro')\n",
    "\n",
    "    Returns:\n",
    "        leverage_scores (dict): 그룹별 손실 값 저장\n",
    "        overall_mean (float): 모든 그룹의 평균 손실\n",
    "    \"\"\"\n",
    "\n",
    "    def leverage_effect(ts, tau):\n",
    "        \"\"\"\n",
    "        레버리지 효과 계산:\n",
    "        Corr(r_t, r_(t+tau)^2)\n",
    "        \"\"\"\n",
    "        rt = ts[:-tau]            # 뒤에서 tau 개 제외\n",
    "        rt_squared = ts[tau:]**2  # 앞에서 tau 개 제외한 뒤 제곱\n",
    "        return np.corrcoef(rt, rt_squared)[0, 1]  # 상관계수\n",
    "\n",
    "    n_vars = len(real_list)\n",
    "    leverage_scores = {}     # 그룹별 손실 값 저장\n",
    "    all_group_losses = []    # 모든 그룹의 손실 값을 모아 전체 평균 손실 계산\n",
    "\n",
    "    for i in range(n_vars):\n",
    "        real_data = real_list[i]  # shape=(n_real_samples, time_length)\n",
    "        fake_data = fake_list[i]  # shape=(n_fake_samples, time_length)\n",
    "\n",
    "        # (1) Real 각 샘플의 레버리지 효과 (tau=1부터 lags까지)\n",
    "        real_leverage_effects = np.array([\n",
    "            [leverage_effect(ts, tau) for tau in range(1, lags+1)]\n",
    "            for ts in real_data\n",
    "        ])  # shape = (n_real_samples, lags)\n",
    "\n",
    "        # (2) Fake 각 샘플의 레버리지 효과\n",
    "        fake_leverage_effects = np.array([\n",
    "            [leverage_effect(ts, tau) for tau in range(1, lags+1)]\n",
    "            for ts in fake_data\n",
    "        ])  # shape = (n_fake_samples, lags)\n",
    "\n",
    "        # (3) Real의 평균 레버리지 효과 계산\n",
    "        mean_real_leverage = np.mean(real_leverage_effects, axis=0)  # shape=(lags,)\n",
    "\n",
    "        # (4) Fake의 평균 레버리지 효과 계산\n",
    "        mean_fake_leverage = np.mean(fake_leverage_effects, axis=0)  # shape=(lags,)\n",
    "\n",
    "        # (5) Real 평균과 Fake 평균 간의 차이 계산\n",
    "        diff = mean_fake_leverage - mean_real_leverage\n",
    "\n",
    "        # (6) loss_type에 따라 손실 계산\n",
    "        if loss_type == 'mse':\n",
    "            loss_val = np.mean(diff**2)\n",
    "        elif loss_type == 'mae':\n",
    "            loss_val = np.mean(np.abs(diff))\n",
    "        elif loss_type == 'frobenius':\n",
    "            loss_val = np.linalg.norm(diff)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss_type: {loss_type}\")\n",
    "\n",
    "        # (7) 손실 값 저장 (소수점 반올림은 최종 저장 시점에 적용)\n",
    "        leverage_scores[f'Group {i+1}'] = loss_val\n",
    "        all_group_losses.append(loss_val)\n",
    "\n",
    "        # (8) 그룹별 손실 출력 (소수점 반올림)\n",
    "        print(f\"[Group {i+1}] Leverage Loss ({loss_type.upper()}) => {loss_val:.4f}\")\n",
    "\n",
    "    # (9) 전체 그룹의 평균 손실 계산\n",
    "    overall_mean = np.mean(all_group_losses)\n",
    "    print(f\"\\n[Overall] Leverage Loss ({loss_type.upper()}) => {overall_mean:.4f}\")\n",
    "\n",
    "    # (10) 최종 결과를 소수점 4자리로 반올림하여 저장\n",
    "    for group in leverage_scores:\n",
    "        leverage_scores[group] = round(leverage_scores[group], 4)\n",
    "    overall_mean = round(overall_mean, 4)\n",
    "\n",
    "    return leverage_scores, overall_mean\n",
    "\n",
    "# 예시 사용\n",
    "\n",
    "# 손실 계산 (예: 'mse', 'mae', 'frobenius' 중 선택)\n",
    "loss_type = 'mse'  # 'mse', 'mae', 'frobenius'\n",
    "\n",
    "leverage_scores, overall_mean = leverage_effect_loss_mean_vs_mean(\n",
    "    real_list, \n",
    "    fake_list, \n",
    "    lags=lags, \n",
    "    loss_type=loss_type\n",
    ")\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(\"\\n=== Leverage Effect Loss (Mean vs. Mean) ===\")\n",
    "for group, loss in leverage_scores.items():\n",
    "    print(f\"{group}: {loss}\")\n",
    "print(f\"Overall Mean Loss: {overall_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the performance of our model by first generating the price process, apply the prespecified trading strategies and compare the resulting PnL process using the real and fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.strategies import log_return_to_price\n",
    "\n",
    "eval_size = real_data.shape[0]\n",
    "fake_data_torch = torch.from_numpy(fake_data).float().transpose(1, 2)\n",
    "real_data_torch = torch.from_numpy(real_data).float().transpose(1, 2)\n",
    "\n",
    "fake_prices = log_return_to_price(fake_data_torch[:eval_size], train_init_price[:eval_size])\n",
    "real_prices = log_return_to_price(real_data_torch[:eval_size], train_init_price[:eval_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = 'src/evaluation/config.yaml'\n",
    "with open(config_dir) as file:\n",
    "    eval_config = ml_collections.ConfigDict(yaml.safe_load(file))\n",
    "    \n",
    "print(fake_data_torch.shape, real_data_torch.shape, test_init_price.shape)\n",
    "\n",
    "all_positive = (fake_prices > 0).all()\n",
    "if not all_positive:\n",
    "    raise ValueError(\"Sanity Check Failed: Some fake prices are not positive.\")\n",
    "\n",
    "res_dict = {\"var_mean\" : 0., \"es_mean\": 0., \"max_drawback_mean\": 0., \"cumulative_pnl_mean\": 0.,}\n",
    "\n",
    "# Do final evaluation\n",
    "num_strat = 4\n",
    "\n",
    "with torch.no_grad():\n",
    "    for strat_name in ['equal_weight', 'mean_reversion', 'trend_following', 'vol_trading']:\n",
    "        subres_dict = full_evaluation(fake_prices, real_prices, eval_config, strat_name = strat_name)\n",
    "        filtered_means = {k: round(v, 4) for k, v in subres_dict.items() if '_mean' in k}\n",
    "        print(strat_name, filtered_means)\n",
    "        for k in res_dict:\n",
    "            res_dict[k] += subres_dict[k] / num_strat \n",
    "for k, v in res_dict.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare real and synthetic data for evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from src.evaluations.plot import *\n",
    "from src.evaluations.evaluations import *\n",
    "val_set = TensorDataset(real_data_torch)\n",
    "fake_set = TensorDataset(fake_data_torch)\n",
    "\n",
    "fake_test_dl = DataLoader(\n",
    "    fake_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "real_test_dl = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "x_real, x_fake = loader_to_tensor(real_test_dl), loader_to_tensor(fake_test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(real_test_dl))[0].shape)\n",
    "print(next(iter(fake_test_dl))[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(real_test_dl, fake_test_dl, config, plot_show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stylised facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plot_hists_marginals(x_real, x_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above illustrate the marginal distribution comparisons across various time steps.\n",
    "Essentially we can quantify this by the marginal distribution loss: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal_loss = to_numpy(HistoLoss(x_real[:, 1:, :], n_bins=50, name='marginal_distribution')(x_fake[:, 1:, :]))\n",
    "# print('Marginal Loss = ', Marginal_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'Our Model'\n",
    "# compare_acf_matrix(real_test_dl, fake_test_dl, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots demonstrated the quality of the generated sample in terms of autocorrelation. Correspondingly, we have the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acf_loss = to_numpy(ACFLoss(x_real, name='auto_correlation')(x_fake))\n",
    "# print('autocorrelation Loss = ', acf_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CrossCorrelLoss 인스턴스 생성 시 name 인수를 추가\n",
    "# cross_corr_loss = CrossCorrelLoss(x_real, max_lag=64, name=\"cross_correlation\")\n",
    "\n",
    "# # 생성 데이터와 비교하여 손실 계산\n",
    "# loss_value = cross_corr_loss.compute(x_fake)\n",
    "# print('Cross-correlation Loss = ', loss_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CovLoss 인스턴스 생성\n",
    "# cov_loss = CovLoss(x_real, name=\"covariance_loss\")\n",
    "\n",
    "# # 생성 데이터와 비교하여 손실 계산\n",
    "# loss_value = cov_loss.compute(x_fake)\n",
    "# print('Covariance Loss = ', loss_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_prices_set = TensorDataset(fake_prices)\n",
    "real_prices_set = TensorDataset(real_prices)\n",
    "\n",
    "fake_prices_dl = DataLoader(\n",
    "    fake_prices_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "real_prices_dl = DataLoader(\n",
    "    real_prices_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tsne_plot(real_prices_dl, fake_prices_dl, config, plot_show =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
