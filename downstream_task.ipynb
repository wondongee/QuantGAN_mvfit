{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspace/COMFI-GAN\n"
     ]
    }
   ],
   "source": [
    "import os, joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from utils import set_seed\n",
    "\n",
    "path = '/workspace/QuantGAN_stock'\n",
    "try:\n",
    "    os.chdir(path)\n",
    "    print(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "except FileNotFoundError:\n",
    "    print(\"Directory {0} does not exist\".format(path))\n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_seed(40)\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # Bidirectional이므로 hidden_size * 2\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.bilstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 timestep의 hidden state를 사용\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "# 모델 초기화 함수\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 (4904, 127)\n",
      "5 (4904, 127)\n"
     ]
    }
   ],
   "source": [
    "# COSCI-GAN, TransGAN 데이터 로드\n",
    "real = joblib.load('./real_list.pkl')\n",
    "fake = joblib.load('./fake_list.pkl')\n",
    "\n",
    "print(len(real), real[0].shape)\n",
    "print(len(fake), fake[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 126개의 시계열 데이터를 사용하여 다음 스텝의 상승/하락 여부를 예측\n",
    "seq_len = 126\n",
    "input_size = 1  # feature 개수\n",
    "hidden_size = 30\n",
    "num_layers = 3\n",
    "lr = 0.003\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "patience = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.6969, Val Loss: 0.6895\n",
      "Epoch [2/200], Loss: 0.6799, Val Loss: 0.6890\n",
      "Epoch [3/200], Loss: 0.6800, Val Loss: 0.6859\n",
      "Epoch [4/200], Loss: 0.6953, Val Loss: 0.6863\n",
      "Epoch [5/200], Loss: 0.6683, Val Loss: 0.6848\n",
      "Epoch [6/200], Loss: 0.7090, Val Loss: 0.6869\n",
      "Epoch [7/200], Loss: 0.6677, Val Loss: 0.6846\n",
      "Epoch [8/200], Loss: 0.6828, Val Loss: 0.6864\n",
      "Epoch [9/200], Loss: 0.6808, Val Loss: 0.6870\n",
      "Epoch [10/200], Loss: 0.6911, Val Loss: 0.6835\n",
      "Epoch [11/200], Loss: 0.6546, Val Loss: 0.6866\n",
      "Epoch [12/200], Loss: 0.6851, Val Loss: 0.6861\n",
      "Epoch [13/200], Loss: 0.6634, Val Loss: 0.6833\n",
      "Epoch [14/200], Loss: 0.6690, Val Loss: 0.6876\n",
      "Epoch [15/200], Loss: 0.7306, Val Loss: 0.6870\n",
      "Epoch [16/200], Loss: 0.6996, Val Loss: 0.6867\n",
      "Epoch [17/200], Loss: 0.6650, Val Loss: 0.6872\n",
      "Epoch [18/200], Loss: 0.6965, Val Loss: 0.6864\n",
      "Epoch [19/200], Loss: 0.7156, Val Loss: 0.6861\n",
      "Epoch [20/200], Loss: 0.6595, Val Loss: 0.6890\n",
      "Epoch [21/200], Loss: 0.6655, Val Loss: 0.6854\n",
      "Epoch [22/200], Loss: 0.6579, Val Loss: 0.6844\n",
      "Epoch [23/200], Loss: 0.6749, Val Loss: 0.6857\n",
      "Epoch [24/200], Loss: 0.6999, Val Loss: 0.6861\n",
      "Epoch [25/200], Loss: 0.6911, Val Loss: 0.6847\n",
      "Epoch [26/200], Loss: 0.6846, Val Loss: 0.6857\n",
      "Epoch [27/200], Loss: 0.6948, Val Loss: 0.6833\n",
      "Epoch [28/200], Loss: 0.7072, Val Loss: 0.6867\n",
      "Epoch [29/200], Loss: 0.6927, Val Loss: 0.6869\n",
      "Epoch [30/200], Loss: 0.6540, Val Loss: 0.6831\n",
      "Epoch [31/200], Loss: 0.6770, Val Loss: 0.6876\n",
      "Epoch [32/200], Loss: 0.6659, Val Loss: 0.6880\n",
      "Epoch [33/200], Loss: 0.6642, Val Loss: 0.6884\n",
      "Epoch [34/200], Loss: 0.6890, Val Loss: 0.6834\n",
      "Epoch [35/200], Loss: 0.6348, Val Loss: 0.6829\n",
      "Epoch [36/200], Loss: 0.6382, Val Loss: 0.6856\n",
      "Epoch [37/200], Loss: 0.6581, Val Loss: 0.6798\n",
      "Epoch [38/200], Loss: 0.6858, Val Loss: 0.6871\n",
      "Epoch [39/200], Loss: 0.6608, Val Loss: 0.6834\n",
      "Epoch [40/200], Loss: 0.6489, Val Loss: 0.6856\n",
      "Epoch [41/200], Loss: 0.6710, Val Loss: 0.6860\n",
      "Epoch [42/200], Loss: 0.6743, Val Loss: 0.6833\n",
      "Epoch [43/200], Loss: 0.6751, Val Loss: 0.6812\n",
      "Epoch [44/200], Loss: 0.6615, Val Loss: 0.6865\n",
      "Epoch [45/200], Loss: 0.6329, Val Loss: 0.6815\n",
      "Epoch [46/200], Loss: 0.6591, Val Loss: 0.6817\n",
      "Epoch [47/200], Loss: 0.6954, Val Loss: 0.6806\n",
      "Epoch [48/200], Loss: 0.6717, Val Loss: 0.6808\n",
      "Epoch [49/200], Loss: 0.6711, Val Loss: 0.6759\n",
      "Epoch [50/200], Loss: 0.6618, Val Loss: 0.6820\n",
      "Epoch [51/200], Loss: 0.6114, Val Loss: 0.6811\n",
      "Epoch [52/200], Loss: 0.6661, Val Loss: 0.6744\n",
      "Epoch [53/200], Loss: 0.6234, Val Loss: 0.6795\n",
      "Epoch [54/200], Loss: 0.6318, Val Loss: 0.6784\n",
      "Epoch [55/200], Loss: 0.6353, Val Loss: 0.6817\n",
      "Epoch [56/200], Loss: 0.6268, Val Loss: 0.6797\n",
      "Epoch [57/200], Loss: 0.6702, Val Loss: 0.6791\n",
      "Epoch [58/200], Loss: 0.6158, Val Loss: 0.6796\n",
      "Epoch [59/200], Loss: 0.6188, Val Loss: 0.6746\n",
      "Epoch [60/200], Loss: 0.6223, Val Loss: 0.6791\n",
      "Epoch [61/200], Loss: 0.6441, Val Loss: 0.6749\n",
      "Epoch [62/200], Loss: 0.6895, Val Loss: 0.6722\n",
      "Epoch [63/200], Loss: 0.6493, Val Loss: 0.6695\n",
      "Epoch [64/200], Loss: 0.6371, Val Loss: 0.6710\n",
      "Epoch [65/200], Loss: 0.5966, Val Loss: 0.6690\n",
      "Epoch [66/200], Loss: 0.6036, Val Loss: 0.6675\n",
      "Epoch [67/200], Loss: 0.6126, Val Loss: 0.6645\n",
      "Epoch [68/200], Loss: 0.5838, Val Loss: 0.6640\n",
      "Epoch [69/200], Loss: 0.6838, Val Loss: 0.6605\n",
      "Epoch [70/200], Loss: 0.6077, Val Loss: 0.6652\n",
      "Epoch [71/200], Loss: 0.5880, Val Loss: 0.6639\n",
      "Epoch [72/200], Loss: 0.6685, Val Loss: 0.6640\n",
      "Epoch [73/200], Loss: 0.6262, Val Loss: 0.6550\n",
      "Epoch [74/200], Loss: 0.5436, Val Loss: 0.6594\n",
      "Epoch [75/200], Loss: 0.6656, Val Loss: 0.6625\n",
      "Epoch [76/200], Loss: 0.6538, Val Loss: 0.6418\n",
      "Epoch [77/200], Loss: 0.6234, Val Loss: 0.6508\n",
      "Epoch [78/200], Loss: 0.6033, Val Loss: 0.6458\n",
      "Epoch [79/200], Loss: 0.6181, Val Loss: 0.6588\n",
      "Epoch [80/200], Loss: 0.6039, Val Loss: 0.6603\n",
      "Epoch [81/200], Loss: 0.5097, Val Loss: 0.6480\n",
      "Epoch [82/200], Loss: 0.5898, Val Loss: 0.6338\n",
      "Epoch [83/200], Loss: 0.5479, Val Loss: 0.6369\n",
      "Epoch [84/200], Loss: 0.6365, Val Loss: 0.6404\n",
      "Epoch [85/200], Loss: 0.6588, Val Loss: 0.6394\n",
      "Epoch [86/200], Loss: 0.5234, Val Loss: 0.6118\n",
      "Epoch [87/200], Loss: 0.5258, Val Loss: 0.6307\n",
      "Epoch [88/200], Loss: 0.4576, Val Loss: 0.6378\n",
      "Epoch [89/200], Loss: 0.6320, Val Loss: 0.6250\n",
      "Epoch [90/200], Loss: 0.5671, Val Loss: 0.6185\n",
      "Epoch [91/200], Loss: 0.4798, Val Loss: 0.6609\n",
      "Epoch [92/200], Loss: 0.5351, Val Loss: 0.6250\n",
      "Epoch [93/200], Loss: 0.4614, Val Loss: 0.6232\n",
      "Epoch [94/200], Loss: 0.3803, Val Loss: 0.6281\n",
      "Epoch [95/200], Loss: 0.4739, Val Loss: 0.6176\n",
      "Epoch [96/200], Loss: 0.3448, Val Loss: 0.6181\n",
      "Epoch [97/200], Loss: 0.6105, Val Loss: 0.6012\n",
      "Epoch [98/200], Loss: 0.3392, Val Loss: 0.6215\n",
      "Epoch [99/200], Loss: 0.4263, Val Loss: 0.6475\n",
      "Epoch [100/200], Loss: 0.5706, Val Loss: 0.6198\n",
      "Epoch [101/200], Loss: 0.4645, Val Loss: 0.6393\n",
      "Epoch [102/200], Loss: 0.6537, Val Loss: 0.6394\n",
      "Epoch [103/200], Loss: 0.4695, Val Loss: 0.6496\n",
      "Epoch [104/200], Loss: 0.5526, Val Loss: 0.6831\n",
      "Epoch [105/200], Loss: 0.4417, Val Loss: 0.6679\n",
      "Epoch [106/200], Loss: 0.3491, Val Loss: 0.6465\n",
      "Epoch [107/200], Loss: 0.2778, Val Loss: 0.7080\n",
      "Epoch [108/200], Loss: 0.3951, Val Loss: 0.6616\n",
      "Epoch [109/200], Loss: 0.3267, Val Loss: 0.6794\n",
      "Epoch [110/200], Loss: 0.3659, Val Loss: 0.6626\n",
      "Epoch [111/200], Loss: 0.3297, Val Loss: 0.7184\n",
      "Epoch [112/200], Loss: 0.2202, Val Loss: 0.7144\n",
      "Epoch [113/200], Loss: 0.3361, Val Loss: 0.6834\n",
      "Epoch [114/200], Loss: 0.3176, Val Loss: 0.7117\n",
      "Epoch [115/200], Loss: 0.2791, Val Loss: 0.6929\n",
      "Epoch [116/200], Loss: 0.2522, Val Loss: 0.7197\n",
      "Epoch [117/200], Loss: 0.2143, Val Loss: 0.7614\n",
      "Epoch [118/200], Loss: 0.3394, Val Loss: 0.7954\n",
      "Epoch [119/200], Loss: 0.2402, Val Loss: 0.7036\n",
      "Epoch [120/200], Loss: 0.1378, Val Loss: 0.8387\n",
      "Epoch [121/200], Loss: 0.4025, Val Loss: 0.9299\n",
      "Epoch [122/200], Loss: 0.1978, Val Loss: 0.7921\n",
      "Epoch [123/200], Loss: 0.2108, Val Loss: 0.7762\n",
      "Epoch [124/200], Loss: 0.1592, Val Loss: 0.7827\n",
      "Epoch [125/200], Loss: 0.1503, Val Loss: 0.9005\n",
      "Epoch [126/200], Loss: 0.1967, Val Loss: 0.8508\n",
      "Epoch [127/200], Loss: 0.1340, Val Loss: 0.8719\n",
      "Early stopping\n",
      "Number of ones: 1977.0\n",
      "Number of zeros: 1575.0\n",
      "Ratio of ones: 0.5566\n",
      "Ratio of zeros: 0.4434\n",
      "Confusion Matrix:\n",
      "[[1067  546]\n",
      " [ 508 1431]]\n",
      "Precision: 0.7238\n",
      "Recall: 0.7380\n",
      "F1 Score: 0.7308\n",
      "Accuracy: 0.7033\n",
      "\n",
      "Epoch [1/200], Loss: 0.6918, Val Loss: 0.6918\n",
      "Epoch [2/200], Loss: 0.6750, Val Loss: 0.6907\n",
      "Epoch [3/200], Loss: 0.6904, Val Loss: 0.6927\n",
      "Epoch [4/200], Loss: 0.6676, Val Loss: 0.6922\n",
      "Epoch [5/200], Loss: 0.6987, Val Loss: 0.6898\n",
      "Epoch [6/200], Loss: 0.6810, Val Loss: 0.6932\n",
      "Epoch [7/200], Loss: 0.6931, Val Loss: 0.6916\n",
      "Epoch [8/200], Loss: 0.6838, Val Loss: 0.6903\n",
      "Epoch [9/200], Loss: 0.6914, Val Loss: 0.6897\n",
      "Epoch [10/200], Loss: 0.6793, Val Loss: 0.6908\n",
      "Epoch [11/200], Loss: 0.6559, Val Loss: 0.6936\n",
      "Epoch [12/200], Loss: 0.6910, Val Loss: 0.6900\n",
      "Epoch [13/200], Loss: 0.6816, Val Loss: 0.6908\n",
      "Epoch [14/200], Loss: 0.6934, Val Loss: 0.6910\n",
      "Epoch [15/200], Loss: 0.6824, Val Loss: 0.6914\n",
      "Epoch [16/200], Loss: 0.6789, Val Loss: 0.6914\n",
      "Epoch [17/200], Loss: 0.6758, Val Loss: 0.6924\n",
      "Epoch [18/200], Loss: 0.6956, Val Loss: 0.6897\n",
      "Epoch [19/200], Loss: 0.7109, Val Loss: 0.6914\n",
      "Epoch [20/200], Loss: 0.6746, Val Loss: 0.6914\n",
      "Epoch [21/200], Loss: 0.6936, Val Loss: 0.6905\n",
      "Epoch [22/200], Loss: 0.7025, Val Loss: 0.6913\n",
      "Epoch [23/200], Loss: 0.6712, Val Loss: 0.6906\n",
      "Epoch [24/200], Loss: 0.6634, Val Loss: 0.6903\n",
      "Epoch [25/200], Loss: 0.6660, Val Loss: 0.6905\n",
      "Epoch [26/200], Loss: 0.6801, Val Loss: 0.6888\n",
      "Epoch [27/200], Loss: 0.6965, Val Loss: 0.6913\n",
      "Epoch [28/200], Loss: 0.6715, Val Loss: 0.6919\n",
      "Epoch [29/200], Loss: 0.6842, Val Loss: 0.6897\n",
      "Epoch [30/200], Loss: 0.6988, Val Loss: 0.6891\n",
      "Epoch [31/200], Loss: 0.6668, Val Loss: 0.6903\n",
      "Epoch [32/200], Loss: 0.6826, Val Loss: 0.6918\n",
      "Epoch [33/200], Loss: 0.6731, Val Loss: 0.6900\n",
      "Epoch [34/200], Loss: 0.7368, Val Loss: 0.6885\n",
      "Epoch [35/200], Loss: 0.6716, Val Loss: 0.6903\n",
      "Epoch [36/200], Loss: 0.6582, Val Loss: 0.6883\n",
      "Epoch [37/200], Loss: 0.6303, Val Loss: 0.6904\n",
      "Epoch [38/200], Loss: 0.6583, Val Loss: 0.6912\n",
      "Epoch [39/200], Loss: 0.7024, Val Loss: 0.6876\n",
      "Epoch [40/200], Loss: 0.6793, Val Loss: 0.6850\n",
      "Epoch [41/200], Loss: 0.7004, Val Loss: 0.6865\n",
      "Epoch [42/200], Loss: 0.6595, Val Loss: 0.6854\n",
      "Epoch [43/200], Loss: 0.6393, Val Loss: 0.6858\n",
      "Epoch [44/200], Loss: 0.6644, Val Loss: 0.6862\n",
      "Epoch [45/200], Loss: 0.6465, Val Loss: 0.6849\n",
      "Epoch [46/200], Loss: 0.6565, Val Loss: 0.6865\n",
      "Epoch [47/200], Loss: 0.6577, Val Loss: 0.6864\n",
      "Epoch [48/200], Loss: 0.6733, Val Loss: 0.6832\n",
      "Epoch [49/200], Loss: 0.6992, Val Loss: 0.6742\n",
      "Epoch [50/200], Loss: 0.6362, Val Loss: 0.6726\n",
      "Epoch [51/200], Loss: 0.7037, Val Loss: 0.6750\n",
      "Epoch [52/200], Loss: 0.6632, Val Loss: 0.6755\n",
      "Epoch [53/200], Loss: 0.6575, Val Loss: 0.6747\n",
      "Epoch [54/200], Loss: 0.6510, Val Loss: 0.6675\n",
      "Epoch [55/200], Loss: 0.5986, Val Loss: 0.6811\n",
      "Epoch [56/200], Loss: 0.7011, Val Loss: 0.6770\n",
      "Epoch [57/200], Loss: 0.6781, Val Loss: 0.6679\n",
      "Epoch [58/200], Loss: 0.6519, Val Loss: 0.6688\n",
      "Epoch [59/200], Loss: 0.5777, Val Loss: 0.6680\n",
      "Epoch [60/200], Loss: 0.6088, Val Loss: 0.6698\n",
      "Epoch [61/200], Loss: 0.6430, Val Loss: 0.6706\n",
      "Epoch [62/200], Loss: 0.6770, Val Loss: 0.6653\n",
      "Epoch [63/200], Loss: 0.6157, Val Loss: 0.6716\n",
      "Epoch [64/200], Loss: 0.5941, Val Loss: 0.6567\n",
      "Epoch [65/200], Loss: 0.6309, Val Loss: 0.6663\n",
      "Epoch [66/200], Loss: 0.5713, Val Loss: 0.6523\n",
      "Epoch [67/200], Loss: 0.5380, Val Loss: 0.6600\n",
      "Epoch [68/200], Loss: 0.5803, Val Loss: 0.6663\n",
      "Epoch [69/200], Loss: 0.6005, Val Loss: 0.6512\n",
      "Epoch [70/200], Loss: 0.5938, Val Loss: 0.6508\n",
      "Epoch [71/200], Loss: 0.4949, Val Loss: 0.6677\n",
      "Epoch [72/200], Loss: 0.5884, Val Loss: 0.6624\n",
      "Epoch [73/200], Loss: 0.5139, Val Loss: 0.6496\n",
      "Epoch [74/200], Loss: 0.6662, Val Loss: 0.7142\n",
      "Epoch [75/200], Loss: 0.5868, Val Loss: 0.6513\n",
      "Epoch [76/200], Loss: 0.5932, Val Loss: 0.6468\n",
      "Epoch [77/200], Loss: 0.4953, Val Loss: 0.6424\n",
      "Epoch [78/200], Loss: 0.5492, Val Loss: 0.6531\n",
      "Epoch [79/200], Loss: 0.5530, Val Loss: 0.6528\n",
      "Epoch [80/200], Loss: 0.6508, Val Loss: 0.6485\n",
      "Epoch [81/200], Loss: 0.4709, Val Loss: 0.6509\n",
      "Epoch [82/200], Loss: 0.5454, Val Loss: 0.6369\n",
      "Epoch [83/200], Loss: 0.5122, Val Loss: 0.6786\n",
      "Epoch [84/200], Loss: 0.5894, Val Loss: 0.6362\n",
      "Epoch [85/200], Loss: 0.5152, Val Loss: 0.7014\n",
      "Epoch [86/200], Loss: 0.4504, Val Loss: 0.6593\n",
      "Epoch [87/200], Loss: 0.4417, Val Loss: 0.7088\n",
      "Epoch [88/200], Loss: 0.4757, Val Loss: 0.6315\n",
      "Epoch [89/200], Loss: 0.3835, Val Loss: 0.6703\n",
      "Epoch [90/200], Loss: 0.6031, Val Loss: 0.6789\n",
      "Epoch [91/200], Loss: 0.4621, Val Loss: 0.6923\n",
      "Epoch [92/200], Loss: 0.4828, Val Loss: 0.6602\n",
      "Epoch [93/200], Loss: 0.5496, Val Loss: 0.7241\n",
      "Epoch [94/200], Loss: 0.4652, Val Loss: 0.6491\n",
      "Epoch [95/200], Loss: 0.4365, Val Loss: 0.6842\n",
      "Epoch [96/200], Loss: 0.3969, Val Loss: 0.8032\n",
      "Epoch [97/200], Loss: 0.3847, Val Loss: 0.6915\n",
      "Epoch [98/200], Loss: 0.3482, Val Loss: 0.7144\n",
      "Epoch [99/200], Loss: 0.2952, Val Loss: 0.7111\n",
      "Epoch [100/200], Loss: 0.2942, Val Loss: 0.6837\n",
      "Epoch [101/200], Loss: 0.2243, Val Loss: 0.8410\n",
      "Epoch [102/200], Loss: 0.3566, Val Loss: 0.7281\n",
      "Epoch [103/200], Loss: 0.3036, Val Loss: 0.7925\n",
      "Epoch [104/200], Loss: 0.3143, Val Loss: 0.7038\n",
      "Epoch [105/200], Loss: 0.3954, Val Loss: 0.7848\n",
      "Epoch [106/200], Loss: 0.3462, Val Loss: 0.8993\n",
      "Epoch [107/200], Loss: 0.3159, Val Loss: 0.8094\n",
      "Epoch [108/200], Loss: 0.3437, Val Loss: 0.8925\n",
      "Epoch [109/200], Loss: 0.2643, Val Loss: 0.8137\n",
      "Epoch [110/200], Loss: 0.3073, Val Loss: 0.8827\n",
      "Epoch [111/200], Loss: 0.2851, Val Loss: 0.8198\n",
      "Epoch [112/200], Loss: 0.5490, Val Loss: 0.8730\n",
      "Epoch [113/200], Loss: 0.2230, Val Loss: 0.9039\n",
      "Epoch [114/200], Loss: 0.2742, Val Loss: 0.8894\n",
      "Epoch [115/200], Loss: 0.2824, Val Loss: 0.9460\n",
      "Epoch [116/200], Loss: 0.3115, Val Loss: 0.9642\n",
      "Epoch [117/200], Loss: 0.2124, Val Loss: 0.8632\n",
      "Epoch [118/200], Loss: 0.2393, Val Loss: 0.9207\n",
      "Early stopping\n",
      "Number of ones: 1818.0\n",
      "Number of zeros: 1734.0\n",
      "Ratio of ones: 0.5118\n",
      "Ratio of zeros: 0.4882\n",
      "Confusion Matrix:\n",
      "[[1102  503]\n",
      " [ 632 1315]]\n",
      "Precision: 0.7233\n",
      "Recall: 0.6754\n",
      "F1 Score: 0.6985\n",
      "Accuracy: 0.6805\n",
      "\n",
      "Epoch [1/200], Loss: 0.6886, Val Loss: 0.6895\n",
      "Epoch [2/200], Loss: 0.6845, Val Loss: 0.6901\n",
      "Epoch [3/200], Loss: 0.6899, Val Loss: 0.6908\n",
      "Epoch [4/200], Loss: 0.6855, Val Loss: 0.6900\n",
      "Epoch [5/200], Loss: 0.6928, Val Loss: 0.6893\n",
      "Epoch [6/200], Loss: 0.6842, Val Loss: 0.6904\n",
      "Epoch [7/200], Loss: 0.6910, Val Loss: 0.6900\n",
      "Epoch [8/200], Loss: 0.7030, Val Loss: 0.6904\n",
      "Epoch [9/200], Loss: 0.6924, Val Loss: 0.6898\n",
      "Epoch [10/200], Loss: 0.6941, Val Loss: 0.6890\n",
      "Epoch [11/200], Loss: 0.6938, Val Loss: 0.6897\n",
      "Epoch [12/200], Loss: 0.6760, Val Loss: 0.6916\n",
      "Epoch [13/200], Loss: 0.6883, Val Loss: 0.6896\n",
      "Epoch [14/200], Loss: 0.6849, Val Loss: 0.6872\n",
      "Epoch [15/200], Loss: 0.6853, Val Loss: 0.6888\n",
      "Epoch [16/200], Loss: 0.6905, Val Loss: 0.6957\n",
      "Epoch [17/200], Loss: 0.6762, Val Loss: 0.6905\n",
      "Epoch [18/200], Loss: 0.6681, Val Loss: 0.6873\n",
      "Epoch [19/200], Loss: 0.6563, Val Loss: 0.6866\n",
      "Epoch [20/200], Loss: 0.7043, Val Loss: 0.6875\n",
      "Epoch [21/200], Loss: 0.7365, Val Loss: 0.6879\n",
      "Epoch [22/200], Loss: 0.7033, Val Loss: 0.6847\n",
      "Epoch [23/200], Loss: 0.6890, Val Loss: 0.6861\n",
      "Epoch [24/200], Loss: 0.6970, Val Loss: 0.6832\n",
      "Epoch [25/200], Loss: 0.6954, Val Loss: 0.6832\n",
      "Epoch [26/200], Loss: 0.6965, Val Loss: 0.6833\n",
      "Epoch [27/200], Loss: 0.6658, Val Loss: 0.6839\n",
      "Epoch [28/200], Loss: 0.6942, Val Loss: 0.6854\n",
      "Epoch [29/200], Loss: 0.6892, Val Loss: 0.6824\n",
      "Epoch [30/200], Loss: 0.6586, Val Loss: 0.6818\n",
      "Epoch [31/200], Loss: 0.6703, Val Loss: 0.6831\n",
      "Epoch [32/200], Loss: 0.6903, Val Loss: 0.6815\n",
      "Epoch [33/200], Loss: 0.6802, Val Loss: 0.6818\n",
      "Epoch [34/200], Loss: 0.6578, Val Loss: 0.6814\n",
      "Epoch [35/200], Loss: 0.6897, Val Loss: 0.6721\n",
      "Epoch [36/200], Loss: 0.6588, Val Loss: 0.6740\n",
      "Epoch [37/200], Loss: 0.6244, Val Loss: 0.6668\n",
      "Epoch [38/200], Loss: 0.6931, Val Loss: 0.6823\n",
      "Epoch [39/200], Loss: 0.6935, Val Loss: 0.6784\n",
      "Epoch [40/200], Loss: 0.6593, Val Loss: 0.6611\n",
      "Epoch [41/200], Loss: 0.6419, Val Loss: 0.6647\n",
      "Epoch [42/200], Loss: 0.5801, Val Loss: 0.6707\n",
      "Epoch [43/200], Loss: 0.6584, Val Loss: 0.6679\n",
      "Epoch [44/200], Loss: 0.6438, Val Loss: 0.6622\n",
      "Epoch [45/200], Loss: 0.6389, Val Loss: 0.6787\n",
      "Epoch [46/200], Loss: 0.6349, Val Loss: 0.6673\n",
      "Epoch [47/200], Loss: 0.6224, Val Loss: 0.6611\n",
      "Epoch [48/200], Loss: 0.6655, Val Loss: 0.6643\n",
      "Epoch [49/200], Loss: 0.5793, Val Loss: 0.6890\n",
      "Epoch [50/200], Loss: 0.6554, Val Loss: 0.6740\n",
      "Epoch [51/200], Loss: 0.6416, Val Loss: 0.6885\n",
      "Epoch [52/200], Loss: 0.6644, Val Loss: 0.6718\n",
      "Epoch [53/200], Loss: 0.6680, Val Loss: 0.7141\n",
      "Epoch [54/200], Loss: 0.5843, Val Loss: 0.6944\n",
      "Epoch [55/200], Loss: 0.5594, Val Loss: 0.6559\n",
      "Epoch [56/200], Loss: 0.5715, Val Loss: 0.7009\n",
      "Epoch [57/200], Loss: 0.5459, Val Loss: 0.6920\n",
      "Epoch [58/200], Loss: 0.6052, Val Loss: 0.6755\n",
      "Epoch [59/200], Loss: 0.5539, Val Loss: 0.6572\n",
      "Epoch [60/200], Loss: 0.5540, Val Loss: 0.6797\n",
      "Epoch [61/200], Loss: 0.4199, Val Loss: 0.6947\n",
      "Epoch [62/200], Loss: 0.5460, Val Loss: 0.6644\n",
      "Epoch [63/200], Loss: 0.4593, Val Loss: 0.7344\n",
      "Epoch [64/200], Loss: 0.5882, Val Loss: 0.6720\n",
      "Epoch [65/200], Loss: 0.5812, Val Loss: 0.7653\n",
      "Epoch [66/200], Loss: 0.4821, Val Loss: 0.7395\n",
      "Epoch [67/200], Loss: 0.4995, Val Loss: 0.6573\n",
      "Epoch [68/200], Loss: 0.4769, Val Loss: 0.7675\n",
      "Epoch [69/200], Loss: 0.4922, Val Loss: 0.7476\n",
      "Epoch [70/200], Loss: 0.4534, Val Loss: 0.7738\n",
      "Epoch [71/200], Loss: 0.4664, Val Loss: 0.7233\n",
      "Epoch [72/200], Loss: 0.4625, Val Loss: 0.7084\n",
      "Epoch [73/200], Loss: 0.4637, Val Loss: 0.7795\n",
      "Epoch [74/200], Loss: 0.3306, Val Loss: 0.8706\n",
      "Epoch [75/200], Loss: 0.3376, Val Loss: 0.7055\n",
      "Epoch [76/200], Loss: 0.4130, Val Loss: 0.8025\n",
      "Epoch [77/200], Loss: 0.5935, Val Loss: 0.8186\n",
      "Epoch [78/200], Loss: 0.3133, Val Loss: 0.8764\n",
      "Epoch [79/200], Loss: 0.4203, Val Loss: 0.9643\n",
      "Epoch [80/200], Loss: 0.3007, Val Loss: 0.8034\n",
      "Epoch [81/200], Loss: 0.3613, Val Loss: 0.9754\n",
      "Epoch [82/200], Loss: 0.3718, Val Loss: 0.9165\n",
      "Epoch [83/200], Loss: 0.3902, Val Loss: 0.9214\n",
      "Epoch [84/200], Loss: 0.3342, Val Loss: 0.9066\n",
      "Epoch [85/200], Loss: 0.4881, Val Loss: 0.8426\n",
      "Early stopping\n",
      "Number of ones: 1699.0\n",
      "Number of zeros: 1853.0\n",
      "Ratio of ones: 0.4783\n",
      "Ratio of zeros: 0.5217\n",
      "Confusion Matrix:\n",
      "[[1200  530]\n",
      " [ 653 1169]]\n",
      "Precision: 0.6881\n",
      "Recall: 0.6416\n",
      "F1 Score: 0.6640\n",
      "Accuracy: 0.6669\n",
      "\n",
      "Epoch [1/200], Loss: 0.6802, Val Loss: 0.6910\n",
      "Epoch [2/200], Loss: 0.6879, Val Loss: 0.6913\n",
      "Epoch [3/200], Loss: 0.6814, Val Loss: 0.6910\n",
      "Epoch [4/200], Loss: 0.6844, Val Loss: 0.6915\n",
      "Epoch [5/200], Loss: 0.6691, Val Loss: 0.6910\n",
      "Epoch [6/200], Loss: 0.7012, Val Loss: 0.6896\n",
      "Epoch [7/200], Loss: 0.6841, Val Loss: 0.6891\n",
      "Epoch [8/200], Loss: 0.6876, Val Loss: 0.6884\n",
      "Epoch [9/200], Loss: 0.6915, Val Loss: 0.6904\n",
      "Epoch [10/200], Loss: 0.6896, Val Loss: 0.6903\n",
      "Epoch [11/200], Loss: 0.6731, Val Loss: 0.6911\n",
      "Epoch [12/200], Loss: 0.6789, Val Loss: 0.6875\n",
      "Epoch [13/200], Loss: 0.6894, Val Loss: 0.6877\n",
      "Epoch [14/200], Loss: 0.6833, Val Loss: 0.6890\n",
      "Epoch [15/200], Loss: 0.6991, Val Loss: 0.6875\n",
      "Epoch [16/200], Loss: 0.6995, Val Loss: 0.6870\n",
      "Epoch [17/200], Loss: 0.6961, Val Loss: 0.6871\n",
      "Epoch [18/200], Loss: 0.6878, Val Loss: 0.6876\n",
      "Epoch [19/200], Loss: 0.6675, Val Loss: 0.6834\n",
      "Epoch [20/200], Loss: 0.6608, Val Loss: 0.6786\n",
      "Epoch [21/200], Loss: 0.7078, Val Loss: 0.6818\n",
      "Epoch [22/200], Loss: 0.6671, Val Loss: 0.6835\n",
      "Epoch [23/200], Loss: 0.7160, Val Loss: 0.6813\n",
      "Epoch [24/200], Loss: 0.6723, Val Loss: 0.6821\n",
      "Epoch [25/200], Loss: 0.6328, Val Loss: 0.7021\n",
      "Epoch [26/200], Loss: 0.6478, Val Loss: 0.6832\n",
      "Epoch [27/200], Loss: 0.6807, Val Loss: 0.6807\n",
      "Epoch [28/200], Loss: 0.6781, Val Loss: 0.6799\n",
      "Epoch [29/200], Loss: 0.6995, Val Loss: 0.6768\n",
      "Epoch [30/200], Loss: 0.6746, Val Loss: 0.6745\n",
      "Epoch [31/200], Loss: 0.6628, Val Loss: 0.6815\n",
      "Epoch [32/200], Loss: 0.6843, Val Loss: 0.6746\n",
      "Epoch [33/200], Loss: 0.6446, Val Loss: 0.6761\n",
      "Epoch [34/200], Loss: 0.5996, Val Loss: 0.6695\n",
      "Epoch [35/200], Loss: 0.6792, Val Loss: 0.6786\n",
      "Epoch [36/200], Loss: 0.5639, Val Loss: 0.6682\n",
      "Epoch [37/200], Loss: 0.6026, Val Loss: 0.6659\n",
      "Epoch [38/200], Loss: 0.6954, Val Loss: 0.6587\n",
      "Epoch [39/200], Loss: 0.6847, Val Loss: 0.6596\n",
      "Epoch [40/200], Loss: 0.5369, Val Loss: 0.6586\n",
      "Epoch [41/200], Loss: 0.5992, Val Loss: 0.6372\n",
      "Epoch [42/200], Loss: 0.5864, Val Loss: 0.6458\n",
      "Epoch [43/200], Loss: 0.6517, Val Loss: 0.6413\n",
      "Epoch [44/200], Loss: 0.6526, Val Loss: 0.6290\n",
      "Epoch [45/200], Loss: 0.6946, Val Loss: 0.6196\n",
      "Epoch [46/200], Loss: 0.7069, Val Loss: 0.6042\n",
      "Epoch [47/200], Loss: 0.5489, Val Loss: 0.6231\n",
      "Epoch [48/200], Loss: 0.7099, Val Loss: 0.6369\n",
      "Epoch [49/200], Loss: 0.5198, Val Loss: 0.6042\n",
      "Epoch [50/200], Loss: 0.5751, Val Loss: 0.6469\n",
      "Epoch [51/200], Loss: 0.5576, Val Loss: 0.6026\n",
      "Epoch [52/200], Loss: 0.4739, Val Loss: 0.6016\n",
      "Epoch [53/200], Loss: 0.4842, Val Loss: 0.6475\n",
      "Epoch [54/200], Loss: 0.5332, Val Loss: 0.6073\n",
      "Epoch [55/200], Loss: 0.3951, Val Loss: 0.6127\n",
      "Epoch [56/200], Loss: 0.4066, Val Loss: 0.5992\n",
      "Epoch [57/200], Loss: 0.3673, Val Loss: 0.6318\n",
      "Epoch [58/200], Loss: 0.3800, Val Loss: 0.6221\n",
      "Epoch [59/200], Loss: 0.3577, Val Loss: 0.6456\n",
      "Epoch [60/200], Loss: 0.2721, Val Loss: 0.7482\n",
      "Epoch [61/200], Loss: 0.2444, Val Loss: 0.7156\n",
      "Epoch [62/200], Loss: 0.2714, Val Loss: 0.6942\n",
      "Epoch [63/200], Loss: 0.2122, Val Loss: 0.6896\n",
      "Epoch [64/200], Loss: 0.4232, Val Loss: 0.7490\n",
      "Epoch [65/200], Loss: 0.2080, Val Loss: 0.7960\n",
      "Epoch [66/200], Loss: 0.1520, Val Loss: 0.8302\n",
      "Epoch [67/200], Loss: 0.3174, Val Loss: 0.8603\n",
      "Epoch [68/200], Loss: 0.1643, Val Loss: 0.8290\n",
      "Epoch [69/200], Loss: 0.1795, Val Loss: 0.8481\n",
      "Epoch [70/200], Loss: 0.2929, Val Loss: 0.7992\n",
      "Epoch [71/200], Loss: 0.1222, Val Loss: 0.9094\n",
      "Epoch [72/200], Loss: 0.2419, Val Loss: 0.8441\n",
      "Epoch [73/200], Loss: 0.1377, Val Loss: 0.9135\n",
      "Epoch [74/200], Loss: 0.0959, Val Loss: 0.9855\n",
      "Epoch [75/200], Loss: 0.2412, Val Loss: 1.0962\n",
      "Epoch [76/200], Loss: 0.1575, Val Loss: 0.9919\n",
      "Epoch [77/200], Loss: 0.0745, Val Loss: 1.0782\n",
      "Epoch [78/200], Loss: 0.1148, Val Loss: 1.1289\n",
      "Epoch [79/200], Loss: 0.0911, Val Loss: 1.1391\n",
      "Epoch [80/200], Loss: 0.1848, Val Loss: 1.2821\n",
      "Epoch [81/200], Loss: 0.0576, Val Loss: 1.4103\n",
      "Epoch [82/200], Loss: 0.2687, Val Loss: 1.2404\n",
      "Epoch [83/200], Loss: 0.1110, Val Loss: 1.4851\n",
      "Epoch [84/200], Loss: 0.1069, Val Loss: 1.2020\n",
      "Epoch [85/200], Loss: 0.2958, Val Loss: 1.5023\n",
      "Epoch [86/200], Loss: 0.0251, Val Loss: 1.3848\n",
      "Early stopping\n",
      "Number of ones: 1820.0\n",
      "Number of zeros: 1732.0\n",
      "Ratio of ones: 0.5124\n",
      "Ratio of zeros: 0.4876\n",
      "Confusion Matrix:\n",
      "[[1186  505]\n",
      " [ 546 1315]]\n",
      "Precision: 0.7225\n",
      "Recall: 0.7066\n",
      "F1 Score: 0.7145\n",
      "Accuracy: 0.7041\n",
      "\n",
      "Epoch [1/200], Loss: 0.6943, Val Loss: 0.6912\n",
      "Epoch [2/200], Loss: 0.6865, Val Loss: 0.6911\n",
      "Epoch [3/200], Loss: 0.6914, Val Loss: 0.6913\n",
      "Epoch [4/200], Loss: 0.6879, Val Loss: 0.6903\n",
      "Epoch [5/200], Loss: 0.6871, Val Loss: 0.6907\n",
      "Epoch [6/200], Loss: 0.6895, Val Loss: 0.6917\n",
      "Epoch [7/200], Loss: 0.7019, Val Loss: 0.6917\n",
      "Epoch [8/200], Loss: 0.7020, Val Loss: 0.6907\n",
      "Epoch [9/200], Loss: 0.6927, Val Loss: 0.6921\n",
      "Epoch [10/200], Loss: 0.6987, Val Loss: 0.6892\n",
      "Epoch [11/200], Loss: 0.6989, Val Loss: 0.6904\n",
      "Epoch [12/200], Loss: 0.6927, Val Loss: 0.6914\n",
      "Epoch [13/200], Loss: 0.6792, Val Loss: 0.6920\n",
      "Epoch [14/200], Loss: 0.7329, Val Loss: 0.6890\n",
      "Epoch [15/200], Loss: 0.6908, Val Loss: 0.6884\n",
      "Epoch [16/200], Loss: 0.7027, Val Loss: 0.6921\n",
      "Epoch [17/200], Loss: 0.6922, Val Loss: 0.6914\n",
      "Epoch [18/200], Loss: 0.6889, Val Loss: 0.6907\n",
      "Epoch [19/200], Loss: 0.6990, Val Loss: 0.6917\n",
      "Epoch [20/200], Loss: 0.6759, Val Loss: 0.6901\n",
      "Epoch [21/200], Loss: 0.6897, Val Loss: 0.6915\n",
      "Epoch [22/200], Loss: 0.6847, Val Loss: 0.6893\n",
      "Epoch [23/200], Loss: 0.7309, Val Loss: 0.6911\n",
      "Epoch [24/200], Loss: 0.6799, Val Loss: 0.6915\n",
      "Epoch [25/200], Loss: 0.7354, Val Loss: 0.6892\n",
      "Epoch [26/200], Loss: 0.6923, Val Loss: 0.6919\n",
      "Epoch [27/200], Loss: 0.6991, Val Loss: 0.6914\n",
      "Epoch [28/200], Loss: 0.6445, Val Loss: 0.6900\n",
      "Epoch [29/200], Loss: 0.6920, Val Loss: 0.6901\n",
      "Epoch [30/200], Loss: 0.6731, Val Loss: 0.6899\n",
      "Epoch [31/200], Loss: 0.6931, Val Loss: 0.6909\n",
      "Epoch [32/200], Loss: 0.6723, Val Loss: 0.6897\n",
      "Epoch [33/200], Loss: 0.7139, Val Loss: 0.6906\n",
      "Epoch [34/200], Loss: 0.6670, Val Loss: 0.6890\n",
      "Epoch [35/200], Loss: 0.6847, Val Loss: 0.6902\n",
      "Epoch [36/200], Loss: 0.6964, Val Loss: 0.6869\n",
      "Epoch [37/200], Loss: 0.6537, Val Loss: 0.6857\n",
      "Epoch [38/200], Loss: 0.6845, Val Loss: 0.6864\n",
      "Epoch [39/200], Loss: 0.7049, Val Loss: 0.6856\n",
      "Epoch [40/200], Loss: 0.6827, Val Loss: 0.6845\n",
      "Epoch [41/200], Loss: 0.7505, Val Loss: 0.6869\n",
      "Epoch [42/200], Loss: 0.6867, Val Loss: 0.6817\n",
      "Epoch [43/200], Loss: 0.6807, Val Loss: 0.6812\n",
      "Epoch [44/200], Loss: 0.7154, Val Loss: 0.6877\n",
      "Epoch [45/200], Loss: 0.6551, Val Loss: 0.6820\n",
      "Epoch [46/200], Loss: 0.6418, Val Loss: 0.6770\n",
      "Epoch [47/200], Loss: 0.6480, Val Loss: 0.6752\n",
      "Epoch [48/200], Loss: 0.6667, Val Loss: 0.6812\n",
      "Epoch [49/200], Loss: 0.6716, Val Loss: 0.6725\n",
      "Epoch [50/200], Loss: 0.6697, Val Loss: 0.6844\n",
      "Epoch [51/200], Loss: 0.6781, Val Loss: 0.6781\n",
      "Epoch [52/200], Loss: 0.6393, Val Loss: 0.6682\n",
      "Epoch [53/200], Loss: 0.6691, Val Loss: 0.6708\n",
      "Epoch [54/200], Loss: 0.7411, Val Loss: 0.6689\n",
      "Epoch [55/200], Loss: 0.6724, Val Loss: 0.6706\n",
      "Epoch [56/200], Loss: 0.5816, Val Loss: 0.6702\n",
      "Epoch [57/200], Loss: 0.6343, Val Loss: 0.6683\n",
      "Epoch [58/200], Loss: 0.5997, Val Loss: 0.6522\n",
      "Epoch [59/200], Loss: 0.6095, Val Loss: 0.6563\n",
      "Epoch [60/200], Loss: 0.6476, Val Loss: 0.6499\n",
      "Epoch [61/200], Loss: 0.5836, Val Loss: 0.6609\n",
      "Epoch [62/200], Loss: 0.5915, Val Loss: 0.6460\n",
      "Epoch [63/200], Loss: 0.6293, Val Loss: 0.6488\n",
      "Epoch [64/200], Loss: 0.6201, Val Loss: 0.6510\n",
      "Epoch [65/200], Loss: 0.5903, Val Loss: 0.6317\n",
      "Epoch [66/200], Loss: 0.6302, Val Loss: 0.6419\n",
      "Epoch [67/200], Loss: 0.5442, Val Loss: 0.6466\n",
      "Epoch [68/200], Loss: 0.6208, Val Loss: 0.6304\n",
      "Epoch [69/200], Loss: 0.6418, Val Loss: 0.6213\n",
      "Epoch [70/200], Loss: 0.6208, Val Loss: 0.6232\n",
      "Epoch [71/200], Loss: 0.5709, Val Loss: 0.6289\n",
      "Epoch [72/200], Loss: 0.5885, Val Loss: 0.6580\n",
      "Epoch [73/200], Loss: 0.5262, Val Loss: 0.6237\n",
      "Epoch [74/200], Loss: 0.4606, Val Loss: 0.6422\n",
      "Epoch [75/200], Loss: 0.5399, Val Loss: 0.6022\n",
      "Epoch [76/200], Loss: 0.6079, Val Loss: 0.6256\n",
      "Epoch [77/200], Loss: 0.5278, Val Loss: 0.5871\n",
      "Epoch [78/200], Loss: 0.4688, Val Loss: 0.5782\n",
      "Epoch [79/200], Loss: 0.4817, Val Loss: 0.5936\n",
      "Epoch [80/200], Loss: 0.4034, Val Loss: 0.6194\n",
      "Epoch [81/200], Loss: 0.4399, Val Loss: 0.5779\n",
      "Epoch [82/200], Loss: 0.4774, Val Loss: 0.5691\n",
      "Epoch [83/200], Loss: 0.3130, Val Loss: 0.6364\n",
      "Epoch [84/200], Loss: 0.4829, Val Loss: 0.5929\n",
      "Epoch [85/200], Loss: 0.4964, Val Loss: 0.6496\n",
      "Epoch [86/200], Loss: 0.4130, Val Loss: 0.6201\n",
      "Epoch [87/200], Loss: 0.3754, Val Loss: 0.6402\n",
      "Epoch [88/200], Loss: 0.4597, Val Loss: 0.6045\n",
      "Epoch [89/200], Loss: 0.3162, Val Loss: 0.6221\n",
      "Epoch [90/200], Loss: 0.4585, Val Loss: 0.5909\n",
      "Epoch [91/200], Loss: 0.3482, Val Loss: 0.6151\n",
      "Epoch [92/200], Loss: 0.2342, Val Loss: 0.5936\n",
      "Epoch [93/200], Loss: 0.3298, Val Loss: 0.7223\n",
      "Epoch [94/200], Loss: 0.3855, Val Loss: 0.6318\n",
      "Epoch [95/200], Loss: 0.2324, Val Loss: 0.5892\n",
      "Epoch [96/200], Loss: 0.3728, Val Loss: 0.5886\n",
      "Epoch [97/200], Loss: 0.2885, Val Loss: 0.6431\n",
      "Epoch [98/200], Loss: 0.3734, Val Loss: 0.6816\n",
      "Epoch [99/200], Loss: 0.1803, Val Loss: 0.6664\n",
      "Epoch [100/200], Loss: 0.2423, Val Loss: 0.6249\n",
      "Epoch [101/200], Loss: 0.2066, Val Loss: 0.6600\n",
      "Epoch [102/200], Loss: 0.2443, Val Loss: 0.7007\n",
      "Epoch [103/200], Loss: 0.1492, Val Loss: 0.6801\n",
      "Epoch [104/200], Loss: 0.2342, Val Loss: 0.7108\n",
      "Epoch [105/200], Loss: 0.2119, Val Loss: 0.7002\n",
      "Epoch [106/200], Loss: 0.1556, Val Loss: 0.6205\n",
      "Epoch [107/200], Loss: 0.3395, Val Loss: 0.8111\n",
      "Epoch [108/200], Loss: 0.1485, Val Loss: 0.7250\n",
      "Epoch [109/200], Loss: 0.2083, Val Loss: 0.7426\n",
      "Epoch [110/200], Loss: 0.2461, Val Loss: 0.7042\n",
      "Epoch [111/200], Loss: 0.2266, Val Loss: 0.8320\n",
      "Epoch [112/200], Loss: 0.1237, Val Loss: 0.8896\n",
      "Early stopping\n",
      "Number of ones: 1842.0\n",
      "Number of zeros: 1710.0\n",
      "Ratio of ones: 0.5186\n",
      "Ratio of zeros: 0.4814\n",
      "Confusion Matrix:\n",
      "[[1193  496]\n",
      " [ 517 1346]]\n",
      "Precision: 0.7307\n",
      "Recall: 0.7225\n",
      "F1 Score: 0.7266\n",
      "Accuracy: 0.7148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(real)):       \n",
    "    # real 데이터 준비\n",
    "    X_real = real[i][:, :seq_len]\n",
    "    targets_real = real[i][:, seq_len]\n",
    "    y_real = (targets_real > 0).astype(int)\n",
    "    \n",
    "    # fake 데이터 준비\n",
    "    X_fake = fake[i][:, :seq_len]\n",
    "    targets_fake = fake[i][:, seq_len]\n",
    "    y_fake = (targets_fake > 0).astype(int)\n",
    "    \n",
    "    # 데이터 스케일링\n",
    "    scaler = StandardScaler()\n",
    "    X_fake = scaler.fit_transform(X_fake)\n",
    "    X_real = scaler.transform(X_real)\n",
    "\n",
    "    # real 데이터의 길이를 계산하여 validation과 test로 나누기\n",
    "    num_real_samples = X_real.shape[0]\n",
    "    val_size = num_real_samples // 4\n",
    "\n",
    "    X_val_real = X_real[:val_size]\n",
    "    y_val_real = y_real[:val_size]\n",
    "\n",
    "    X_test_real = X_real[val_size + seq_len:]\n",
    "    y_test_real = y_real[val_size + seq_len:]\n",
    "    \n",
    "    # NumPy 배열을 PyTorch 텐서로 변환\n",
    "    X_fake = torch.tensor(X_fake, dtype=torch.float32).unsqueeze(2).to(device)\n",
    "    y_fake = torch.tensor(y_fake, dtype=torch.float32).to(device)\n",
    "    X_val_real = torch.tensor(X_val_real, dtype=torch.float32).unsqueeze(2).to(device)\n",
    "    y_val_real = torch.tensor(y_val_real, dtype=torch.float32).to(device)\n",
    "    X_test_real = torch.tensor(X_test_real, dtype=torch.float32).unsqueeze(2).to(device)\n",
    "    y_test_real = torch.tensor(y_test_real, dtype=torch.float32).to(device)\n",
    "\n",
    "    # TensorDataset과 DataLoader를 사용하여 데이터셋을 배치로 나누기\n",
    "    train_dataset = TensorDataset(X_fake, y_fake)\n",
    "    val_dataset = TensorDataset(X_val_real, y_val_real)\n",
    "    test_dataset = TensorDataset(X_test_real, y_test_real)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = BiLSTMClassifier(input_size, hidden_size, num_layers).to(device)\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    best_model = model.state_dict().copy()  # 초기 모델 상태 저장\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    # 학습 중 loss와 val_loss 값을 저장할 리스트 초기화\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # 학습 시작\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs.squeeze(), y)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient Clipping\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "        # Epoch당 평균 학습 손실 저장\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                val_outputs = model(x_val)\n",
    "                val_loss += criterion(val_outputs.squeeze(), y_val).item()        \n",
    "        val_loss /= len(val_loader)            \n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping and model saving\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    # 모델 평가\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    else:\n",
    "        print(\"No improvement, using the last model.\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for x_test, y_test in test_loader:\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            outputs = model(x_test).squeeze()            \n",
    "            predictions = (outputs >= 0.5).float()\n",
    "            all_predictions.append(predictions)\n",
    "            all_targets.append(y_test)\n",
    "        \n",
    "        # 모든 배치의 결과를 하나로 합침\n",
    "        all_predictions = torch.cat(all_predictions).cpu()\n",
    "        all_targets = torch.cat(all_targets).cpu()        \n",
    "\n",
    "    # 1과 0의 비율 계산\n",
    "    num_ones = all_predictions.sum().item()\n",
    "    num_zeros = len(all_predictions) - num_ones\n",
    "    ratio_ones = num_ones / len(all_predictions)\n",
    "    ratio_zeros = num_zeros / len(all_predictions)\n",
    "\n",
    "    print(f\"Number of ones: {num_ones}\")\n",
    "    print(f\"Number of zeros: {num_zeros}\")\n",
    "    print(f\"Ratio of ones: {ratio_ones:.4f}\")\n",
    "    print(f\"Ratio of zeros: {ratio_zeros:.4f}\")\n",
    "        \n",
    "    # 혼동 행렬\n",
    "    conf_matrix = confusion_matrix(all_targets.numpy(), all_predictions.numpy())\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # 정밀도, 재현율, F1 점수\n",
    "    precision = precision_score(all_targets.numpy(), all_predictions.numpy())\n",
    "    recall = recall_score(all_targets.numpy(), all_predictions.numpy())\n",
    "    f1 = f1_score(all_targets.numpy(), all_predictions.numpy())\n",
    "    accuracy = accuracy_score(all_targets.numpy(), all_predictions.numpy())\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # # Loss 그래프 그리기\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # plt.plot(train_losses, label='Training Loss')\n",
    "    # plt.plot(val_losses, label='Validation Loss')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.legend()\n",
    "    # plt.title('Training and Validation Loss')\n",
    "    # plt.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
